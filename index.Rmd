---
title: "Computational Musicology -- Zoe Wong"
date: "2021/22"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
---
```{r, include=FALSE}
library(compmus)
library(ggplot2)
library(plotly)
library(tidyverse)
library(spotifyr)
library(tidymodels)
library(ggdendro)
library(heatmaply)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  
```

```{r, include=FALSE}
canto <- get_playlist_audio_features("", "57WgAlUaNcdk6En5fD5IeZ")
mandarin <- get_playlist_audio_features("", "05GSdBKp8ZZwlVA4czOFrM")
korean <- get_playlist_audio_features("", "0tFjLsZ50AsfehSNdl6YOc")
english <- get_playlist_audio_features("", "1tFMVzk46FL7SKkNvDftFh")
taiwanese <- get_playlist_audio_features("","3Q0ZWuIxIybBYql1xRCDZi")
japan <- get_playlist_audio_features("", "14vCVdgEYVQ8p1ewq5twRq")
awards <-
  bind_rows(
    mandarin %>% mutate(category = "Mandarin"),
    korean %>% mutate(category = "Korean"),
    canto %>% mutate(category = "Cantonese"),
    japan %>% mutate(category = "Japanese"),
    english %>% mutate(category = "English"),
    taiwanese %>% mutate(category = "Taiwanese")
)
blindinglight <-
  get_tidy_audio_analysis("0VjIjW4GlUZAMYd2vXMi3b?si=846801ced0774ffb") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
inyoureyes <-
  get_tidy_audio_analysis("7szuecWAPwGoV1e5vGu8tl?si=66f65d5a10ef4df4") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
```

### Dendrogram
```{r, include=FALSE} 
koreandd <-
  get_playlist_audio_features("", "2zYOiaRahIUZd0YjzCIJ1S") %>%
  add_audio_analysis() %>%
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "euclidean"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```

```{r}
koreandd_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      liveness +
      valence +
      tempo +
      duration,
    data = koreandd
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  # step_range(all_predictors()) %>% 
  prep(koreandd %>% mutate(track.name = str_trunc(track.name, 20))) %>%
  juice() %>%
  column_to_rownames("track.name")
```

```{r}
koreandd_dist <- dist(koreandd_juice, method = "euclidean")
```

```{r}
heatmaply(
  koreandd_juice,
  hclustfun = hclust,
  hclust_method = "complete",  # Change for single, average, or complete linkage.
  dist_method = "euclidean",
  main = "Dendrogram of significant tracks from language-based genre [13]"
)
```
*Table 13: Dendrogram of significant tracks from language-based genre*

***

In order to find the similarity between each language-based genre, I select two typical tracks from each pop in terms of Spotify API features. There are some conditions when making selection within each pop:
(1) most of the API values lie on interquartile range (between Q1-Q3);
(2) they have sufficient track popularity (~50 or above);
(3) they comply with the dominating music genre within their language corpus

Thus, these selected tracks are able to represent their genres in order to achieve better comparison between each category. 

(I am still discovering ways to show better presentation in terms of language pop, as the track name of each songs are in their own languages and it is really inconvenient for English or non-Asian viewers)

From table 3, the dendrogram presents the magnitude of the Spotify API features (neglecting key and instrumentalness) and their clusters:

The audio feature of "Only You(Taiwanese)[12th]" are the outermost from other tracks, where the numeral value or the colour goes relatively extreme, such as the dark blue areas of loudness and energy and the yellow areas of duration and acousticness. 

However, it cannot represent Taiwanese as the outlining genre. Another Taiwanese track, "Love is Great(Taiwanese)[7]" and its cluster with "See you soon(Cantonese)[8th]" has the second-high similarities, while the cluster of "MAGO (Korean)[9th]" and "Moonlanding (Mandarin)[10th]" has the highest similarity in Spotify API features.

"Levitating(English)[3rd]", "Run to the night(Japanese)[4th]","Dancing Feet(English)[1st]" and "Savage(Korean)[2nd] are categorized into one cluster and relatively heterogeneous with other tracks excluding "Only You(Taiwanese)[12th]".

**We can conclude that these tracks are inter-correlating with each language pop. To a large extent, most tracks result a higher(yellowish) value in loudness, energy, valence and danceability, and a lower(darker value) in liveness and acousticness.**


### **Introduction**


In this course, I propose a computational analysis of my personal playlist on Spotify. My pocket playlist comprises **disco, dance-pop, and sentimental ballads**, which I have been accumulating since September 2020. These songs are looped or appeared in my pocket list at least for 2 weeks, which proves that they are my **favourites during my university life**.

The collection of these songs is solely influenced by my YouTube explore, Spotify recommendations and radio shows. Since my background allows me to access different language-pop over the Asia, the playlist is composed round 90 songs from languages, such as Korean, English, Mandarin, Cantonese, Taiwanese, and Japanese. The diversity of my favourites is inspirational to my project and I hope to understand the mechanism behind my choice.

The analysis would be conducted according to the geographical region/verbal language of the playlist: **United States *(English)*, Korea *(Korean)*, Hong Kong *(Cantonese)*, Taiwan *(Mandarin/Taiwanese)* and Japan *(Japanese)*.** Each region has its distinctive features, such as: danceability, acousticness, valence and tempo; however, each connects each with the combination of its genre and languages, and this project aims to **evaluate their similarities and patterns**. Thus, in each category, both typical and unique tracks are selected, to represent or emphasize their typicality/atypicality.


*Notes:
The written words of Cantonese, Mandarin and Taiwanese are all based on Chinese, in three accents/dialects. In this corpus, I will only focus on Mando-pop in Taiwan*


**** 

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/5Icv4PceG8IB7AiPqSJmNR?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>



  
  
### Language as Genre: Danceability

```{r, echo=FALSE, fig.align ='center'}
danceability_category <- awards %>%
  ggplot(aes(x = category, y = danceability, fill = category)) +
  geom_violin(adjust=1.5, alpha=.5) + geom_jitter(color="black", size=0.4, alpha=0.9) + labs(title =
  "Danceability distribution in different languages [1]")

ggplotly(danceability_category)
 
```

***
*Table 1: Danceability distribution in different languages*

**English** has the highest medium of danceability among all genres, followed by Korean. Both Cantonese and Taiwanese have the even distribution of danceability between 0.4 and 0.8. Mandarin and Japanese have the lower medium of danceability among all genres. Overall, all tracks are higher than 0.5, which symbolizes my tendency in listening music with **high dancebility**

### Language as Genre: **Tempo**


```{r, echo=FALSE}
tempo_category <- awards %>%
  ggplot(aes(x = category, y = tempo, fill = category)) +
  geom_boxplot(adjust=1.5, alpha=.5) + labs(title =
  "Tempo distribution in different languages [2]") 

ggplotly(tempo_category)
```

***
*Table 2: Tempo distribution in different languages*

Vice versa, the distribution in tempo shows an opposite pattern comparing to danceability, where English has the lowest medium, excluding atypical tracks: *Starboy, Blinding Lights* (The Weeknd) and *Stay* (The Kid Laroi ft. Justin Bieber). Along with Cantonese, both distribution are the narrowest between 100 and 130 BPM. **Taiwanese** has the highest medium of tempo at 150 BPM. Overall, most of the medium of tempo distribution locate **around 125 BPM**.







### Language as Genre: Valence

```{r, echo=FALSE}
 valence_category <- awards %>%
  ggplot(aes(x = valence, group = category, fill = category)) +
  geom_density(adjust=1.5, alpha=.3) + labs(title =
  "Valence distribution in different languages [3]") + facet_wrap(~category)

ggplotly(valence_category)
```

***
*Table 3: Valence Distribution in different languages*

The majority of distribution have higher tendency of valence, referring that my playlist possesses relatively **music positivity**, such as Cantonese, Japanese and Tawiwanese. However, the tendency of **Mandarin** music goes opposite, whereas there are higher density at 0.3. It illustrates the music negativity of Mandarin tracks in my playlist, which correlates to the sentimental ballads I have chosen.



### Histogram of Key distribution

```{r, echo=FALSE}
key_order <- c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)

key_template <-
  tribble(~name,~template,
    "C", 0,
    "C#/Db", 1,
    "D", 2,
    "D#/Eb", 3,
    "E", 4,
    "F", 5,
    "F#/Gb",6,
    "G", 7,
    "G#/Ab", 8,
    "A", 9,
    "A#/Bb", 10,
    "B", 11
)

key_category <- awards %>%
  mutate(
  ) %>%
  ggplot(aes(x = key, fill = category)) +
  geom_histogram(binwidth = 0.5, adjust=1.5, alpha=.5) + labs(title =
  "Key distribution in different languages [4]")

ggplotly(key_category)

```
***
*Table 4: Key distribution in different languages*

The key of **C and C#/Db** have the highest counts of tracks in my playlist, which English composes most of them. The key of D#/Eb and E appear as the least count where Mandarin composes most of them. It also shows the key distribution between languages, such as **Korean has the evenest distribution.** 



### Comparison: Chroma features of The Weeknd

```{r, echo=FALSE, fig.show="hold", out.width="50%"}
blindinglight %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Chromagram of 'Blinding light' by The Weeknd [5]") +
  theme_minimal() +
  scale_fill_viridis_c()

inyoureyes %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Chromagram of 'In your eyes' by The Weeknd [6]") +
  theme_minimal() +
  scale_fill_viridis_c()


```

***
*Table 5: Chromagram of "Blinding Light" (The Weeknd)* 

From table 5, we can see that **F key** has a consistent pattern of its timbre feature in "Blinding Light". We might hardly recognize the chorus on the chromagram; however tracing back to the track, a strong rhythmic echo in F key clearly stands out as the bass of the song. We can also see a cycle throughout the entire song, where F key goes to C, then C# and keep circulating.

*Table 6: Chromagram of "In Your Eyes" (The Weeknd)* 

From table 6, the euclidean timbre features show the **route of the bass, from G to C to F and back to G.** It is relatively obvious compared to above, with many sudden change between chords, and it is not the chorus but the bass of the track.

By comparing these 2 songs from the same artist, we can see that **C and F chords** often appear as the key timbre.



### Self-Similarity Matrix of "I Can't Stop me"
```{r, include=FALSE}
icsm <-
  get_tidy_audio_analysis("37ZtpRBkHcaq6hHy0X98zn?si=9181c96d417c4ec3") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "manhattan"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "manhattan"              # Change summary & norm.
      )
  )
```

```{r, echo=FALSE, fig.show="hold", out.width="50%"}
timbre_icsm <- icsm %>%
  compmus_self_similarity(timbre, "cosine") %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Timbre cepstogram of 'I can't stop me' [7]")
timbre_icsm


pitches_icsm <- icsm %>%
  compmus_self_similarity(pitches, "cosine") %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "", title = "Pitches cepstogram of 'I can't stop me' [8]")
 pitches_icsm
```

***
*Table 7: Timbre of "I Can't Stop Me" (Twice)*

From table 7, it displays a few blue chequerboard and yellow outlines. Such as between 40-60 secs, 100-120 secs and 160-180 secs, the chequerboards are identicial, symbolizing the pre-chorus of track. The yellow grid lines appear when human voices exist; vice versa, such as 60-80 and 170-190 secs, these are the chorus with pure instrumental.

*Table 8: Pitches of "I Can't Stop Me" (Twice)*

From table 8, we can see that the pitch class of "I Can't Stop Me" are consistent throughout the track, with densed diagonal lines. The yellow grid lines clearly divide the track into parts. If we zoom into 0-5 secs, 35-40 secs, 75 secs, 100 secs, 140-150 secs and 190-200 secs, these are the moments/periods where pitches transpose. 

Overall, we can analyze the structure of "I Can't Stop Me": Intro -> Verse -> Pre-chorus -> chorus -> 
Verse -> Pre-chorus -> chorus -> Bridge -> pre-chorus -> chorus -> outro [ABABCB].

((i cant show both diagram on the same page as it overlaps, problem to be solved))



### Chordogram of 'Yoru ni kakeru (Run to the night)'

```{r, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )
key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```
```{r, include=FALSE}
yoru <-
  get_tidy_audio_analysis("3dPtXHP0oXQ4HCWHsOA9js?si=d016c5d1d5174bf1") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```

```{r, echo=FALSE}
yoru_graph <- yoru %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "cosine",  # Try different distance metrics
    norm = "chebyshev"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "", title = "[9] Chordogram of 'Yoru ni kakeru (Run to the night)' by YOASOBI")

ggplotly(yoru_graph)
```

***
*Table 9: Chordogram of "Yoru ni kakeru 夜に駆ける " (Run to the night)*

We can see two obvious yellow vertical lines at 15-30 and 105-120 secs, these are the chorus without human voices. If we zoom into the bridge between 195-205 secs,we can see that few of the blue boxes transpose to Bmin/Dmaj/F#min/Amaj while two of yellow boxes transpose to Ebmaj/Abmaj. According to the track, the bridge and ending chorus have transposed to a higher key. It might nor clearly state in the chordogram, but we can see the blue box transpose from C minor to Cmaj/Amin/Fmaj/Dmin (actually the track reaches A minor).




### Tempogram

```{r, echo=FALSE, fig.show="hold", out.width="50%"}
O.O <- 
  get_tidy_audio_analysis("3lrNsPdn98i6rxO142pLT6?si=0bf5176da3534b3a")

O.O %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Tempogram of 'O.O' by NMIXX [11]") +
  theme_classic()


nextlevel <- 
  get_tidy_audio_analysis("2zrhoHlFKxFTRF5aMyxMoQ?si=8e5fabfe220e49f9")

nextlevel %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title = "Tempogram of 'Next Level' by Aespa [12]") +
  theme_classic()


```

***


*Table 11: Tempogram of "O.O" by NMIXX*

*Table 12: Tempogram of "Next Level" by Aespa*

**Both tracks are K-pop and belongs to joint-tracks, which refers to that two or more styles of music/chorus/verses are combined into one track. "O.O" and "Aespa" are two of the recent joint-tracks I really addicted to, where their structures are constructed by two or more verses.** 

From table 11, we can see the tempo at the beginning revolves around 125-135 BPM, followed by sudden drop to around 105 BPM, and keeps accelerating to 135 BPM. The first verse enters around 35-40 secs and stay constant until 70 secs. The second verse enters then at 100 BPM and stay until 110 secs. Between 120-140 secs, the tempo level jumps between 100 BPM and around 145 BPM, which can conclude as the bridge (the track guides the audience back to the original verse). After the emerging bridge, it return back to the tempo level in the first verse, and continues till the end. We can see a clear pattern of how the track jumps between verses, proving that it is a joint-track with distinctive tempo.

From table 12, the tempo starts at 110 BPM and stay constant until around 110 sec. Then, the yellow lines separate into two: around 90 BPM and around 140 BPM. It can be explained by the corresponding tempo of two timbre feature: the line at 90 BPM is relatively vivid as it represents the bass instrument of the verse; the line at 140 BPM is lighter as it represents another supporting instrument (acting as to accelerate to climax). Around 160 secs, the tempo joins together at around 110 BPM, symbolizing the return of the first section/verse. Similar to table 11, the tempogram clearly divide the track into three parts [ABA].

However, if we look closely to table 12, it delivers a more precise structure
